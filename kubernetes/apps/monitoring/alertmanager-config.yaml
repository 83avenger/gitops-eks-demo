---
# AlertManager Configuration — Route alerts to correct channels
apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-config
  namespace: monitoring
data:
  alertmanager.yml: |
    global:
      resolve_timeout: 5m
      slack_api_url: "${SLACK_WEBHOOK_URL}"   # From External Secret

    # Alert templates
    templates:
      - /etc/alertmanager/templates/*.tmpl

    route:
      group_by: ['alertname', 'cluster', 'service', 'namespace']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 4h
      receiver: 'default'

      routes:
        # P1 - Critical: Wake someone up
        - match:
            severity: critical
          receiver: pagerduty-critical
          continue: true   # Also send to Slack

        # P2 - Warning: Slack only
        - match:
            severity: warning
          receiver: slack-warning

        # Watchdog: silence the heartbeat alert
        - match:
            alertname: Watchdog
          receiver: blackhole

    receivers:
      - name: 'default'
        slack_configs:
          - channel: '#alerts-general'
            title: '{{ template "slack.title" . }}'
            text: '{{ template "slack.text" . }}'
            send_resolved: true

      - name: 'pagerduty-critical'
        pagerduty_configs:
          - routing_key: "${PAGERDUTY_ROUTING_KEY}"
            description: '{{ template "pagerduty.description" . }}'
            severity: '{{ if .Labels.severity }}{{ .Labels.severity }}{{ else }}critical{{ end }}'
            details:
              namespace: '{{ .Labels.namespace }}'
              pod: '{{ .Labels.pod }}'
              runbook: '{{ .Annotations.runbook_url }}'

      - name: 'slack-warning'
        slack_configs:
          - channel: '#alerts-warning'
            title: '⚠️ {{ .CommonLabels.alertname }}'
            text: >-
              {{ range .Alerts }}
              *Namespace:* {{ .Labels.namespace }}
              *Alert:* {{ .Annotations.summary }}
              *Description:* {{ .Annotations.description }}
              *Runbook:* {{ .Annotations.runbook_url }}
              {{ end }}
            send_resolved: true

      - name: 'blackhole'
        # Swallows alerts (for Watchdog heartbeat)

---
# Critical Alert Rules
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: gitops-demo-alerts
  namespace: monitoring
  labels:
    prometheus: kube-prometheus
    role: alert-rules
spec:
  groups:
    - name: availability
      interval: 30s
      rules:
        # SLO: 99.9% availability — fires when error budget burning too fast
        - alert: HighErrorRate
          expr: |
            sum(rate(http_requests_total{status=~"5.."}[5m])) by (service, namespace)
            /
            sum(rate(http_requests_total[5m])) by (service, namespace)
            > 0.01
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "High error rate on {{ $labels.service }}"
            description: "Error rate is {{ $value | humanizePercentage }} on {{ $labels.service }} in {{ $labels.namespace }}"
            runbook_url: "https://github.com/YOUR_ORG/runbooks/high-error-rate"

        - alert: HighLatency
          expr: |
            histogram_quantile(0.99,
              sum(rate(http_request_duration_seconds_bucket[5m])) by (service, le)
            ) > 1.0
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "High P99 latency on {{ $labels.service }}"
            description: "P99 latency is {{ $value }}s on {{ $labels.service }}"

    - name: pods
      rules:
        - alert: PodCrashLooping
          expr: |
            rate(kube_pod_container_status_restarts_total[15m]) * 60 * 15 > 3
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping"
            runbook_url: "https://github.com/YOUR_ORG/runbooks/pod-crash-loop"

        - alert: PodNotReady
          expr: |
            kube_pod_status_ready{condition="true"} == 0
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "Pod {{ $labels.namespace }}/{{ $labels.pod }} not ready for 10m"

    - name: resources
      rules:
        - alert: NodeMemoryPressure
          expr: |
            (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) > 0.85
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "Node {{ $labels.node }} memory > 85%"

        - alert: PVCStorageLow
          expr: |
            (1 - (kubelet_volume_stats_available_bytes / kubelet_volume_stats_capacity_bytes)) > 0.80
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "PVC {{ $labels.namespace }}/{{ $labels.persistentvolumeclaim }} is 80% full"
